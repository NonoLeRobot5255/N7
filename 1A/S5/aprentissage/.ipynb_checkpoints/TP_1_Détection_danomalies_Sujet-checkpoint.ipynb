{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xkr9IiKZe-ZW"
   },
   "source": [
    "# Détection de trajectoires anormales pour la surveillance maritime\n",
    "\n",
    "La détection d'anomalies, est une problématique importante en apprentissage automatique et analyse de données. Le concept d'anomalie fait référence à des observations ou des comportements atypiques par rapport à la norme, souvent susceptibles d'indiquer des situations inattendues ou des incidents potentiellement préjudiciables. Pendant les 2 prochaines séances, nous explorerons diverses approches de détection d'anomalies, que nous illustrerons sur des données de trajectoires de navires.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://drive.google.com/uc?id=1m7Tar-_SAsEM05wvhK5Nf72Q11h-etpL\">\n",
    "</center>\n",
    "\n",
    "Ces trajectoires, représentent des déplacements de navires à travers une séquences de 16 coordonnées $(x, y)$, et peuvent révéler des schémas de comportement normaux, mais également des situations anormales telles que des déviations de routes ou des activités inattendues en mer.\n",
    "\n",
    "Nous approcherons de problème de deux manières différentes : la détection d'anomalies supervisée dans un premier temps (TP 1), et la détection d'anomalies non-supervisée pour le TP 2.\n",
    "\n",
    "Le premier TP consistera en un rappel des notions vues en Analyse de Données au 1er semestre, et à l'introduction de l'environnement de travail pour les séances de TP de ce semestre, à savoir Python à travers les librairies *NumPy*, *Scikit-learn* (et plus tard, *Keras* et *Tensorflow*). Nous commencerons par projeter les données dans un espace à plus faible dimension à l'aide de l'Analyse en Composante Principales, puis nous testerons deux algorithmes d'apprentissage supervisé, à savoir les Support Vector Machines (SVM) et les forêts aléatoires. Ces méthodes s'appuient sur des **ensembles d'entraînement étiquetés** pour apprendre à discriminer les trajectoires normales des anomalies.\n",
    "\n",
    "Dans le second volet, nous nous tournerons vers des approches de détection d'anomalies non-supervisées. Nous examinerons l'utilisation de LOF (Local Outlier Factor), un algorithme basé sur la densité locale, ainsi que One-Class SVM, une méthode qui apprend à définir la région normale de l'espace des données sans nécessiter d'exemples d'anomalies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EfCNit-UYBCm"
   },
   "source": [
    "## Préambule : la librairie NumPy\n",
    "\n",
    "\n",
    "**NumPy** est une bibliothèque fondamentale en Python dédiée principalement à la manipulation et au calcul numérique. Elle offre une structure de données appelée `NumPy array` qui est au cœur de son fonctionnement. Voici quelques points clés sur le fonctionnement de la bibliothèque NumPy et le type `NumPy array` :\n",
    "\n",
    "Le `NumPy array` est une structure de données multidimensionnelle qui permet de stocker des données de manière efficace.\n",
    "Il peut être utilisé pour représenter des tableaux unidimensionnels (vecteurs), bidimensionnels (matrices), ou même de dimensions supérieures.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eP7iY19tYxqC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Création de numpy arrays\n",
    "array1d = np.array([1, 2, 3, 4, 5])\n",
    "matrix2d = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Vecteur :\", array1d)\n",
    "print(\"Matrice :\\n\", matrix2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCoYbIHxZzrM"
   },
   "source": [
    "Comme dans Matlab, il est possible d'accéder à des éléments de la structure de données, seuls ou en groupe :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h3CZZElYZ9Z6"
   },
   "outputs": [],
   "source": [
    "# Accès aux éléments\n",
    "element_at_index_2 = array1d[2]\n",
    "print(\"Elément à l'indice 2 du vecteur :\", element_at_index_2)\n",
    "\n",
    "last_element = array1d[-1]\n",
    "print(\"Dernier élément du vecteur :\", last_element)\n",
    "\n",
    "element_at_row_1_col_2 = matrix2d[1, 2]\n",
    "print(\"Elément à la ligne 1, colonne 2 de la matrice :\", element_at_row_1_col_2)\n",
    "\n",
    "col_2 = matrix2d[:,2]\n",
    "print(\"Colonne 2 de la matrice :\", col_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDCRgVsDaOZP"
   },
   "source": [
    "**Prenez garde : en Python, les indices commencent à 0 et pas à 1 comme en Matlab !!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XNagmRsZmhV"
   },
   "source": [
    "\n",
    "NumPy fournit des fonctions pour effectuer des opérations mathématiques et statistiques sur les numpy arrays.\n",
    "Ces opérations sont optimisées, améliorant ainsi la performance par rapport aux boucles traditionnelles en Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1xDbPBITa8Pz"
   },
   "outputs": [],
   "source": [
    "# Fonctions statistiques\n",
    "mean_value = np.mean(array1d)\n",
    "print(\"Valeur moyenne du vecteur : \", mean_value)\n",
    "max_value = np.max(matrix2d)\n",
    "print(\"Valeur maximale de la matrice : \", max_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSeQ_HHKb1Af"
   },
   "source": [
    "Toutes les fonctions d'algèbre linéaire sont également implémentées, par exemple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-GbY1MSLZonf"
   },
   "outputs": [],
   "source": [
    "# Algèbre linéaire\n",
    "eigenvalues, eigenvectors = np.linalg.eig(matrix2d)\n",
    "\n",
    "print(\"Valeurs propres :\", eigenvalues)\n",
    "print(\"Vecteurs propres : \\n\", eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sO0zJB3Cb-YO"
   },
   "source": [
    "Pendant les TP, je vous indiquerai toujours les fonctions NumPy pertinentes à utiliser, en revanche charge à vous d'aller visiter la [documentation](https://numpy.org/doc/stable/index.html) pour trouver comment appeler ces fonctions !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_gjVJ0CfLgS"
   },
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKRdy-b26xTv"
   },
   "source": [
    "Commençons par récupérer les données au format texte sur l'URL suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KTgvfUuP3jLV"
   },
   "outputs": [],
   "source": [
    "!wget -O donnees.txt \"https://drive.google.com/uc?export=download&id=1pBTRdWAX98p_FPm41x3F9z0BOGow4egu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vq8Mrm8TcVWg"
   },
   "source": [
    "Ce fichier contient les données au format texte : chaque ligne correspond à une trajectoire de navire.\n",
    "\n",
    "Une trajectoire est définie par une séquence de 16 paires de coordonnées $(x_i, y_i)$, $i=1..16$.\n",
    "\n",
    "Chaque trajectoire constitue une ligne du fichier texte, organisée comme suit : $x_1, x_2, ..., x_{16}, y_1, y_2, ..., y_{16}$\n",
    "\n",
    "Les 250 premières trajectoires correspondent à des trajectoires normales, et les 10 dernières sont des anomalies.\n",
    "\n",
    "**Travail à faire :** Complétez le code ci-dessous, en utilisant notamment la fonction `shape` de la librairie NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6MsU-tNZfSlD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Lecture de la base de données et mise en forme des données\n",
    "def read_dataset(file_path):\n",
    "    data = np.loadtxt(file_path) # Lecture des données dans un numpy array de dimension (260, 32)\n",
    "\n",
    "    # Nombre de trajectoires de la base de données\n",
    "    # A COMPLETER\n",
    "    num_trajectories = ...\n",
    "\n",
    "    # Séparation des trajectoires normales et anormales (anomalies)\n",
    "    # A COMPLETER\n",
    "    X_normal = ...\n",
    "    X_anomaly = ...\n",
    "\n",
    "    return X_normal, X_anomaly\n",
    "\n",
    "X_normal, X_anomaly = read_dataset('donnees.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4I5_Dz-dHt_"
   },
   "source": [
    "La fonction suivante permet d'afficher les trajectoires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NkGpGg5g5wd3"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_trajectories(normal_trajectories, anomaly_trajectories):\n",
    "    # Affichage des trajectoires normales en vert\n",
    "    plt.plot([], [], color='green', linewidth=0.5, label='Trajectoires normales')\n",
    "    for traj in normal_trajectories:\n",
    "        # Les 16 premières coordonnées sont les abscisses des points de la trajectoire\n",
    "        x = traj[:16]\n",
    "        # Les 16 coordonnées suivantes sont les ordonnées des points de la trajectoire\n",
    "        y = traj[16:]\n",
    "        plt.plot(x, y, color='green', linewidth=0.5)\n",
    "\n",
    "\n",
    "    # Affichage des trajectoires anormales en rouge et pointillés\n",
    "    plt.plot([], [], color='red', linestyle='dashed', linewidth=0.5, label='Trajectoires anormales')\n",
    "    for traj in anomaly_trajectories:\n",
    "        x = traj[:16]\n",
    "        y = traj[16:]\n",
    "        plt.plot(x, y, color='red', linestyle='dashed', linewidth=0.5)\n",
    "\n",
    "    # Labels d'axes et titre\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title('Trajectoires de navires')\n",
    "\n",
    "    # Couleur de fond et grille\n",
    "    plt.gca().set_facecolor('lightgrey')\n",
    "    plt.grid(color='white', linestyle='-', linewidth=0.5)\n",
    "\n",
    "    # Ajout de la légende\n",
    "    plt.legend()\n",
    "\n",
    "    # Affichage de la figure\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_trajectories(X_normal, X_anomaly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thlhBHDXewC9"
   },
   "source": [
    "## TP 1 - Détection d'anomalies supervisée\n",
    "\n",
    "Ce premier TP constitue à la fois une mise en route dans l'environnement Python, et une révision des notions vues en Analyse de Données au 1er semestre.\n",
    "\n",
    "On se place donc dans le cadre de l'apprentissage supervisé : on suppose que nos données sont étiquetées. Ainsi, à chaque donnée $x$ (une trajectoire), on peut associer une étiquette (ou un label) $y$ : 0 si c'est une anomalie, 1 si la trajectoire est normale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i6nyRzXWfx8I"
   },
   "outputs": [],
   "source": [
    "X = np.concatenate([X_normal, X_anomaly])\n",
    "Y = np.concatenate([np.ones(X_normal.shape[0]), np.zeros(X_anomaly.shape[0])])\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EX422FVZgWxM"
   },
   "source": [
    "On se place donc dans le cadre de la classification binaire, pour laquelle nous avons vu plusieurs méthodes au 1er semestre : classification bayésienne, SVM ou encore arbres de décision.\n",
    "\n",
    "Le schéma général d'une application d'apprentissage supervisé est le suivant :\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1qY6sq_1hXWugsGKad2dl26LdYFyyX-o5\" width=600>\n",
    "\n",
    "A partir de l'acquisition d'un motif (\"forme\") par un capteur, on réalise un pré-traitement dont le but est souvent de diminuer la dimension de l'espace des données. Une phase d'apprentissage est réalisée pour entraîner un modèle, qui sert ensuite à prendre des décisions sur de nouvelles données (dans une phase dite d'inférence).\n",
    "\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1ZkrUAmCMMWiwWWoY_lw9sAe8vCS2ExA6\" width=600>\n",
    "\n",
    "\n",
    "Dans ce TP, nous classifierons des trajectoires de navire dont nous allons préalablement réduire la dimension grâce à l'ACP. Puis nous appliquerons un SVM pour apprendre à séparer les trajectoires normales des anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3xBjIJu7qB0"
   },
   "source": [
    "### 1.1 - Analyse en Composantes Principales\n",
    "\n",
    "L'analyse en composantes principales est une technique de réduction de dimension dont l'objectif est de projeter les données dans un espace de faible dimension grâce à une transformation linéaire. Cette projection est réalisé avec l'objectif de maximiser la dispersion (variance) des données projetées.\n",
    "\n",
    "**Travail à faire :** Complétez la fonction compute_pca qui réalise la projection des données `data` sur les `num_axis` premiers axes de l'ACP. Pour rappel, la base de l'ACP est obtenue en diagonalisant la matrice de variance-covariance des données, et en récupérant les `num_axis` vecteurs propres corresondant aux plus grandes valeurs propres.\n",
    "\n",
    "**Indications :** Pour écrire cette fonction, vous aurez certainement besoin des fonctions NumPy suivantes : `mean`, `cov` (prenez garde à l'argument `rowvar`), `eig`, `argsort` (Attention, le tri est dans l'ordre croissant), `flip` et `matmul`. A vous de lire la documentation de ces fonctions avec attention pour les utiliser corectement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1pBhxW4Uep8A"
   },
   "outputs": [],
   "source": [
    "# Fonction de projection des données sur les num_axis premiers axes de l'ACP\n",
    "# A COMPLETER\n",
    "def compute_pca(data, num_axis=2):\n",
    "    # Etape 1: Centrage des données\n",
    "\n",
    "    # Etape 2: Calcul de la matrice de variance-covariance\n",
    "\n",
    "    # Etape 3: Calcul des couples propres de la matrice de variance-covariance\n",
    "\n",
    "    # Etape 4: Réorganisation des vecteurs propres selon l'ordre des valeurs propres décroissantes\n",
    "\n",
    "    # Etape 5: Récupération des num_axis premiers vecteurs propres\n",
    "\n",
    "    # Etape 6: Projection des données sur les axes de l'ACP\n",
    "\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxQMMySm1ikc"
   },
   "source": [
    "Une fois la fonction écrite, testez là avec le bloc de code suivant et vérifiez que votre ACP est correcte !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W4e_9FyM8rvB"
   },
   "outputs": [],
   "source": [
    "# Projection des trajectoires normales et anormales dans les plan des 2 premiers axes de l'ACP\n",
    "X_pca = compute_pca(X, num_axis=2)\n",
    "\n",
    "# Affichage des trajectoires dans le plan des 2 premiers axes de l'ACP\n",
    "plt.scatter(X_pca[:250, 0], X_pca[:250, 1], color='green', label='Trajectoires normales', alpha=0.5)\n",
    "plt.scatter(X_pca[250:, 0], X_pca[250:, 1], color='red', label='Trajectoires anormales', alpha=0.5)\n",
    "\n",
    "# Labels d'axes et titre\n",
    "plt.xlabel('1e Composante Principale')\n",
    "plt.ylabel('2e Composante Principale')\n",
    "plt.title('ACP des trajectoires')\n",
    "\n",
    "# Légende\n",
    "plt.legend()\n",
    "\n",
    "# Affichage de la figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BZMQNgg7lWj"
   },
   "source": [
    "```\n",
    "Résultat attendu :\n",
    "```\n",
    "<img src=\"https://drive.google.com/uc?id=1k4-OxZrM3muPxI57LX_BKydaCKrePSCq\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EO7MOpZV2WJa"
   },
   "source": [
    "Le résultat est satisfaisant : les anomalies sont assez bien séparées des trajectoires normales dans le plan de l'ACP. Il reste à entraîner un classifieur binaire pour apprendre à séparer les 2 classes. Le classifieur bayésien que nous avons vu au 1er semestre n'est pas très adapté, car ni les trajectoires normales ni les anomalies ne suivent une loi gaussienne. On va donc pour cette classification utiliser un **SVM**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yifgLdiX_RQ9"
   },
   "source": [
    "### 1.2 - Classification des anomalies par SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bt5bnXLp2was"
   },
   "source": [
    "Dans un premier temps, exécutez le bloc de code ci-dessous afin de pouvoir disposer d'une fonction de visualisation de la frontière de décision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MPllBIv__snZ"
   },
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X, y, model, title):\n",
    "    h = .02  # Step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlGn, alpha=0.3)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlGn, edgecolors='k', marker='o')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('1e Composante Principale')\n",
    "    plt.ylabel('2e Composante Principale')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygowUHXP3NE9"
   },
   "source": [
    "Nous allons commencer par séparer notre ensemble de données en un ensemble d'apprentissage (pour entraîner le modèle) et un ensemble de test (pour évaluer la capacité de généralisation du modèle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rIhdRPGE3cQC"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Séparation des données en ensembles d'apprentissage (80% des données) et de test (20%)\n",
    "# Le paramètre stratify assure une bonne représentation des anomalies dans les 2 ensembles\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, Y, test_size=0.2, random_state=5, stratify=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "keFCSdvs28Qw"
   },
   "source": [
    "Les SVM ne sont plus au programme de ce semestre, nous n'allons donc cette fois pas les implémenter nous-même, mais plutôt utiliser la classe `SVC` (pour Support Vector Classification) de la librairie `scikit-learn`.\n",
    "\n",
    "**Travail à faire :**\n",
    "\n",
    "*   Complétez le code ci-dessous pour entraîner un SVM à noyau gaussien (indiquez la valeur 1 pour le paramètre $\\gamma$) et marge souple (utilisez 1 comme valeur de $C$).\n",
    "\n",
    "*   Complétez également les lignes permettant de calculer le nombre de faux/vrais positifs/négatifs afin d'afficher la matrice de confusion du modèle. Pour cela les fonctions `logical_and` et `count_nonzero` pourront vous être utiles.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iXE0GRRH_UwT"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Entraînement d'un modèle SVM avec noyau gaussien et marge souple\n",
    "# A COMPLETER\n",
    "svm_model = ...\n",
    "\n",
    "\n",
    "# Évaluation du modèle sur l'ensemble de test\n",
    "accuracy = svm_model.score(X_test, y_test)\n",
    "print(f\"Pourcentage de bonnes classifications sur l'ensemble de test : {100*accuracy:.2f}%\")\n",
    "\n",
    "# Calcul des prédictions du modèle sur l'ensemble de test\n",
    "y_pred = svm_model.predict(X_test)\n",
    "# A COMPLETER\n",
    "TP = ...\n",
    "TN = ...\n",
    "FP = ...\n",
    "FN = ...\n",
    "\n",
    "print(\"Matrice de confusion :\")\n",
    "print(\"------------------------\")\n",
    "print(\"| pred/true |  0  |  1  |\")\n",
    "print(\"------------------------\")\n",
    "print(f\"|     0     | {TN:3d} | {FN:3d} |\")\n",
    "print(\"------------------------\")\n",
    "print(f\"|     1     | {FP:3d} | {TP:3d} |\")\n",
    "print(\"------------------------\")\n",
    "\n",
    "# Affichage de la frontière de décision\n",
    "plot_decision_boundary(X_train, y_train, svm_model, \"SVM à marge souple et noyau gaussien - Ensemble d'apprentissage\")\n",
    "plot_decision_boundary(X_test, y_test, svm_model, \"SVM à marge souple et noyau gaussien - Ensemble de test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8eLlSWZ7k29"
   },
   "source": [
    "```\n",
    "Résultat attendu :\n",
    "\n",
    "Pourcentage de bonnes classifications sur l'ensemble de test : 98.08%\n",
    "Matrice de confusion :\n",
    "------------------------\n",
    "| pred/true |  0  |  1  |\n",
    "------------------------\n",
    "|     0     |   1 |   0 |\n",
    "------------------------\n",
    "|     1     |   1 |  50 |\n",
    "------------------------\n",
    "```\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1_35wRJXo447re6B1ymmsHLAUGWKn241b\" width=500>\n",
    "<img src=\"https://drive.google.com/uc?id=1XDpmaWfJmSO7T0w7ljd-eE_Sqd0IblPb\" width=500>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hFv5J6k8goG"
   },
   "source": [
    "**Travail à faire :** Observez l'impact des différentes valeurs de $\\gamma$ et $C$ sur la frontière de décision. Testez (au moins) 2 valeurs supplémentaires pour chacun de ces paramètres (un très petit et un très grand)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-POoD0KCscC"
   },
   "source": [
    "### Forêts aléatoires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEA9afKm-VEO"
   },
   "source": [
    "Pour terminer ce TP, testez maintenant l'algorithme des forêts aléatoires sur cette base de données. A vous de trouver comment faire en utilisant la bonne fonction de `scikit-learn` et en réutilisant le code écrit pour le SVM."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
