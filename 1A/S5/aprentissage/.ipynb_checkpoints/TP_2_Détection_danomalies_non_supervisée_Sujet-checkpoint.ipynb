{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xkr9IiKZe-ZW"
   },
   "source": [
    "# Détection de trajectoires anormales pour la surveillance maritime\n",
    "\n",
    "La détection d'anomalies, est une problématique importante en apprentissage automatique et analyse de données. Le concept d'anomalie fait référence à des observations ou des comportements atypiques par rapport à la norme, souvent susceptibles d'indiquer des situations inattendues ou des incidents potentiellement préjudiciables. Pendant les 2 prochaines séances, nous explorerons diverses approches de détection d'anomalies, que nous illustrerons sur des données de trajectoires de navires.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://drive.google.com/uc?id=1m7Tar-_SAsEM05wvhK5Nf72Q11h-etpL\">\n",
    "</center>\n",
    "\n",
    "Ces trajectoires, représentent des déplacements de navires à travers une séquences de 16 coordonnées $(x, y)$, et peuvent révéler des schémas de comportement normaux, mais également des situations anormales telles que des déviations de routes ou des activités inattendues en mer.\n",
    "\n",
    "Nous approcherons de problème de deux manières différentes : la détection d'anomalies supervisée dans un premier temps (TP 1), et la détection d'anomalies non-supervisée pour le TP 2.\n",
    "\n",
    "Le premier TP consistera en un rappel des notions vues en Analyse de Données au 1er semestre, et à l'introduction de l'environnement de travail pour les séances de TP de ce semestre, à savoir Python à travers les librairies *NumPy*, *Scikit-learn* (et plus tard, *Keras* et *Tensorflow*). Nous commencerons par projeter les données dans un espace à plus faible dimension à l'aide de l'Analyse en Composante Principales, puis nous testerons deux algorithmes d'apprentissage supervisé, à savoir les Support Vector Machines (SVM) et les forêts aléatoires. Ces méthodes s'appuient sur des **ensembles d'entraînement étiquetés** pour apprendre à discriminer les trajectoires normales des anomalies.\n",
    "\n",
    "Dans le second volet, nous nous tournerons vers des approches de détection d'anomalies non-supervisées. Nous examinerons l'utilisation de LOF (Local Outlier Factor), un algorithme basé sur la densité locale, ainsi que One-Class SVM, une méthode qui apprend à définir la région normale de l'espace des données sans nécessiter d'exemples d'anomalies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_gjVJ0CfLgS"
   },
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKRdy-b26xTv"
   },
   "source": [
    "Commençons par récupérer les données au format texte sur l'URL suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DrvQucuu4rum"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-26 08:36:01--  http://acarlier.fr/donnees.txt\n",
      "Résolution de acarlier.fr (acarlier.fr)… 212.194.248.69, 2a02:842a:5e:4701:b48a:fe3:257b:3972\n",
      "Connexion à acarlier.fr (acarlier.fr)|212.194.248.69|:80… connecté.\n",
      "requête HTTP transmise, en attente de la réponse… 503 Service Unavailable\n",
      "2024-02-26 08:36:01 erreur 503 : Service Unavailable.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://acarlier.fr/donnees.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vq8Mrm8TcVWg"
   },
   "source": [
    "Ce fichier contient les données au format texte : chaque ligne correspond à une trajectoire de navire.\n",
    "\n",
    "Une trajectoire est définie par une séquence de 16 paires de coordonnées $(x_i, y_i)$, $i=1..16$.\n",
    "\n",
    "Chaque trajectoire constitue une ligne du fichier texte, organisée comme suit : $x_1, x_2, ..., x_{16}, y_1, y_2, ..., y_{16}$\n",
    "\n",
    "Les 250 premières trajectoires correspondent à des trajectoires normales, et les 10 dernières sont des anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6MsU-tNZfSlD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Lecture de la base de données et mise en forme des données\n",
    "def read_dataset(file_path):\n",
    "    data = np.loadtxt(file_path) # Lecture des données dans un numpy array de dimension (260, 32)\n",
    "\n",
    "    # Nombre de trajectoires de la base de données\n",
    "    num_trajectories = data.shape[0]\n",
    "\n",
    "    # Séparation des trajectoires normales et anormales (anomalies)\n",
    "    X_normal = data[:num_trajectories-10]\n",
    "    X_anomaly = data[num_trajectories-10:]\n",
    "\n",
    "\n",
    "    return X_normal, X_anomaly\n",
    "\n",
    "X_normal, X_anomaly = read_dataset('donnees.txt')\n",
    "X = np.concatenate([X_normal, X_anomaly])\n",
    "Y = np.concatenate([np.ones(X_normal.shape[0]), np.zeros(X_anomaly.shape[0])])\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4I5_Dz-dHt_"
   },
   "source": [
    "La fonction suivante permet d'afficher les trajectoires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NkGpGg5g5wd3"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_trajectories(normal_trajectories, anomaly_trajectories):\n",
    "    # Affichage des trajectoires normales en vert\n",
    "    plt.plot([], [], color='green', linewidth=0.5, label='Trajectoires normales')\n",
    "    for traj in normal_trajectories:\n",
    "        # Les 16 premières coordonnées sont les abscisses des points de la trajectoire\n",
    "        x = traj[:16]\n",
    "        # Les 16 coordonnées suivantes sont les ordonnées des points de la trajectoire\n",
    "        y = traj[16:]\n",
    "        plt.plot(x, y, color='green', linewidth=0.5)\n",
    "\n",
    "\n",
    "    # Affichage des trajectoires anormales en rouge et pointillés\n",
    "    plt.plot([], [], color='red', linestyle='dashed', linewidth=0.5, label='Trajectoires anormales')\n",
    "    for traj in anomaly_trajectories:\n",
    "        x = traj[:16]\n",
    "        y = traj[16:]\n",
    "        plt.plot(x, y, color='red', linestyle='dashed', linewidth=0.5)\n",
    "\n",
    "    # Labels d'axes et titre\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title('Trajectoires de navires')\n",
    "\n",
    "    # Couleur de fond et grille\n",
    "    plt.gca().set_facecolor('lightgrey')\n",
    "    plt.grid(color='white', linestyle='-', linewidth=0.5)\n",
    "\n",
    "    # Ajout de la légende\n",
    "    plt.legend()\n",
    "\n",
    "    # Affichage de la figure\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_trajectories(X_normal, X_anomaly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLhkCrBJe25h"
   },
   "source": [
    "## TP 2 - Détection d'anomalies non supervisée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BoJgz--fy4uB"
   },
   "source": [
    "Dans ce second TP, nous allons maintenant considérer que nous ne disposons pas d'ensemble d'apprentissage labellisé, ce qui signifie que nous nous ne savons pas au préalable si certaines des données sont des anomalies ou pas.\n",
    "\n",
    "Commencez par exécutez les blocs ci-dessous, pour reprendre les résultats du TP précédent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m3n6biZnoVFu"
   },
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X, y, model, title):\n",
    "    h = .02  # Step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlGn, alpha=0.3)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlGn, edgecolors='k', marker='o')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('1e Composante Principale')\n",
    "    plt.ylabel('2e Composante Principale')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Fonction de projection des données sur les num_axis premiers axes de l'ACP\n",
    "def compute_pca(data, num_axis=2):\n",
    "    # Etape 1: Centrage des données\n",
    "    mean_values = np.mean(data, axis=0)\n",
    "    centered_data = data - mean_values\n",
    "\n",
    "    # Etape 2: Calcul de la matrice de variance-covariance\n",
    "    covariance_matrix = np.cov(centered_data, rowvar=False)\n",
    "\n",
    "    # Etape 3: Calcul des couples propres de la matrice de variance-covariance\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n",
    "\n",
    "    # Etape 4: Réorganisation des vecteurs propres selon l'ordre des valeurs propres décroissantes\n",
    "    sorted_indices = np.flip(np.argsort(eigenvalues))\n",
    "    sorted_eigenvalues = eigenvalues[sorted_indices]\n",
    "    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "    # Etape 5: Récupération des num_axis premiers vecteurs propres\n",
    "    top_eigenvectors = sorted_eigenvectors[:, :num_axis]\n",
    "\n",
    "    # Etape 6: Projection des données sur les axes de l'ACP\n",
    "    projected_data = np.matmul(centered_data, top_eigenvectors)\n",
    "\n",
    "    return projected_data\n",
    "\n",
    "# Projection des trajectoires normales et anormales dans les plan des 2 premiers axes de l'ACP\n",
    "X_pca = compute_pca(X, num_axis=2)\n",
    "\n",
    "# Affichage des trajectoires dans le plan des 2 premiers axes de l'ACP\n",
    "plt.scatter(X_pca[:250, 0], X_pca[:250, 1], color='green', label='Trajectoires normales', alpha=0.5)\n",
    "plt.scatter(X_pca[250:, 0], X_pca[250:, 1], color='red', label='Trajectoires anormales', alpha=0.5)\n",
    "\n",
    "# Labels d'axes et titre\n",
    "plt.xlabel('1e Composante Principale')\n",
    "plt.ylabel('2e Composante Principale')\n",
    "plt.title('ACP des trajectoires')\n",
    "\n",
    "# Légende\n",
    "plt.legend()\n",
    "\n",
    "# Affichage de la figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqUSWvgUqr17"
   },
   "source": [
    "Le code ci-dessous nous permettra d'évaluer les résultats et d'afficher la matrice de confusion obtenue.\n",
    "\n",
    "**Travail à faire**:\n",
    "Complétez le code ci-dessous pour calculer le nombre de vrais positifs (TP), vrais négatifs (TN), faux positifs (FP) et faux négatifs (FN), ainsi que le pourcentage de bonnes classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SOKMpix-q3Qr"
   },
   "outputs": [],
   "source": [
    "def evaluate(y_pred, y):\n",
    "  # Calcul des prédictions du modèle sur l'ensemble de test\n",
    "  TP = ...\n",
    "  TN = ...\n",
    "  FP = ...\n",
    "  FN = ...\n",
    "\n",
    "  accuracy = ...\n",
    "\n",
    "  print(f\"Pourcentage de bonnes classifications : {100*accuracy:.2f}%\")\n",
    "  print(\"Matrice de confusion :\")\n",
    "  print(\"------------------------\")\n",
    "  print(\"| pred/true |  0  |  1  |\")\n",
    "  print(\"------------------------\")\n",
    "  print(f\"|     0     | {TN:3d} | {FN:3d} |\")\n",
    "  print(\"------------------------\")\n",
    "  print(f\"|     1     | {FP:3d} | {TP:3d} |\")\n",
    "  print(\"------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUUwqydUKbnr"
   },
   "source": [
    "### 1 - DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZ_3uoDvzvBR"
   },
   "source": [
    "Dans un premier temps, nous allons expérimenter un algorithme vu en cours au 1er semestre : l'algorithme DBSCAN (**D**ensity **B**ased **S**patial **C**lustering of **A**pplications with **N**oise). Il s'agit pour mémoire d'un algorithme de clustering, c'est-à-dire de regroupement des données en clusters de données ressemblantes. Toutes les données qui ne sont pas affectées à un cluster peuvent être considérées comme des anomalies.\n",
    "\n",
    "L'algorithme DBSCAN repose sur la définition d'un voisinage autour de chaque donnée, dans un rayon $\\epsilon$. Les données dont le $\\epsilon$-voisinage contient trop peu d'autres données sont considérées comme des anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAeKIE6s0cKr"
   },
   "source": [
    "Le code ci-dessous vous permettra d'exécuter DBSCAN puis d'afficher les points. Les labels renvoyés par l'algorithme sont positifs ou nuls : tous les points labellisés 0 appartiennent à un premier cluster, tous les points labellisés 1 à un 2e cluster, etc. Les points labellisés -1 sont des anomalies (ou des points *Noise* dans la terminologie DBSCAN).\n",
    "\n",
    "**Travail à faire**:\n",
    "-   Complétez le code ci-dessous pour exécuter DBSCAN et afficher les anomalies détectées.\n",
    "-   Prenez ensuite le temps de chercher une valeur du paramètre $\\epsilon$ qui permet d'obtenir les meilleurs résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "06l8seukK6es"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Appel de l'algorithme DBSCAN\n",
    "clustering = ...\n",
    "# Les prédictions sont contenues dans la variable clustering.labels_\n",
    "# Les anomalies (classe 0) correspondent aux points pour lesquels DBSCAN assigne la valeur -1\n",
    "y_pred = ...\n",
    "\n",
    "evaluate(y_pred,Y)\n",
    "\n",
    "# Affichage des trajectoires dans le plan des 2 premiers axes de l'ACP\n",
    "plt.scatter(X_pca[y_pred==1, 0], X_pca[y_pred==1, 1], color='green', label='Trajectoires normales', alpha=0.5)\n",
    "plt.scatter(X_pca[y_pred==0, 0], X_pca[y_pred==0, 1], color='red', label='Trajectoires anormales', alpha=0.5)\n",
    "\n",
    "# Labels d'axes et titre\n",
    "plt.xlabel('1e Composante Principale')\n",
    "plt.ylabel('2e Composante Principale')\n",
    "plt.title('Classification non supervisée des anomalies par DBSCAN')\n",
    "\n",
    "# Légende\n",
    "plt.legend()\n",
    "\n",
    "# Affichage de la figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-UXp9MyMYlr"
   },
   "source": [
    "### 2 - Local Outlier Factor\n",
    "\n",
    "L'algorithme DBSCAN, bien que pouvant donner de bons résultats pour la détection d'anomalies, n'est pas complètement adapté pour la détection d'anomalies. Il ne peut donner qu'une information binaire (oui ou non) sur la possibilité pour une donnée d'être une anomalie.\n",
    "\n",
    "La méthode LOF (**L**ocal **O**utlier **F**actor) vue en cours, permet justement d'améliorer cet aspect. Elle consiste à calculer un score LOF$(x_i)$ associé à chaque donnée. Plus ce score est élevé, plus la donnée a de chance d'être une anomalie.\n",
    "\n",
    "Pour chaque point $x_i$ on commence d'abord par calculer :     \n",
    "\n",
    "$$\\mu(x_i) = \\left(\\frac{1}{|\\mathcal{N}_k(x_i)|}\n",
    "\\sum_{x_j\\in \\mathcal{N}_k(x_i)}\n",
    "d_k(x_i,x_j)\n",
    "\\right)^{-1},\\quad \\text{où } \\mathcal{N}_k(x_i) \\text{ désigne les $k$ plus proches voisins de $x_i$} $$\n",
    "\n",
    "Puis :    \n",
    "$$\\text{LOF}_k(x_i) =\n",
    "\\frac{\\frac{1}{|\\mathcal{N}_k(x_i)|}\n",
    "\\sum_{x_j\\in \\mathcal{N}_k(x_i)}\\mu(x_j)}{\\mu(x_i)} $$\n",
    "\n",
    "\n",
    "On va procéder de la manière suivante\n",
    "\n",
    "1.   Calculer une matrice des distances de tous les points à tous les points\n",
    "2.   Trouver, pour chaque point, les indices de ses k plus proches voisins\n",
    "3.   Calculer $\\mu(x_i)$ pour tous point $x_i$\n",
    "4.   Calculer LOF($x_i$) pour tout point $x_i$\n",
    "5.   Diviser LOF($x_i$) par la valeur LOF maximale de la base de données pour obtenir des valeurs entre 0 et 1.\n",
    "\n",
    "**Complétez le code ci-dessous pour implémenter LOF**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q7gST4fTN7aw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def calculate_lof(X, k=5):\n",
    "\n",
    "  # Calcul de la matrice des distances de tous les points à tous les points\n",
    "\n",
    "  # Détermination des indices des k plus proches de voisins de chaque point\n",
    "\n",
    "  # Calcul de la \"reachability distance\" (mu) de chaque point\n",
    "\n",
    "  # Calcul du LOF pour chaque point\n",
    "\n",
    "  # Normalisation du LOF entre 0 et 1\n",
    "\n",
    "  return lof_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTyuOvI8usYj"
   },
   "source": [
    "Le code ci-dessous permet d'appeler votre fonction LOF.\n",
    "\n",
    "**Travail à faire**:\n",
    "-   Vérifiez qu'avec $k=5$ vous obtenez le même résultat que celui qui vous est fourni.\n",
    "-   Testez différentes valeurs de $k$. Pour quelle valeur obtient-on les meilleurs résultats ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EEg-Sgq4QMmb"
   },
   "outputs": [],
   "source": [
    "result_lof = calculate_lof(X_pca,k=5)\n",
    "\n",
    "# Trouver les indices des 10 points avec les scores LOF les plus élevés\n",
    "top_10_indices = np.argsort(result_lof)[-10:]\n",
    "y_pred = np.ones(Y.shape)\n",
    "y_pred[top_10_indices] = 0\n",
    "\n",
    "evaluate(y_pred,Y)\n",
    "\n",
    "# Affichage des points avec des cercles proportionnels au LOF normalisé\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c='green', s=50, alpha=0.5, label='Points Normaux')\n",
    "\n",
    "# Affichage des 10 points avec les scores LOF les plus élevés en noir avec des cercles\n",
    "plt.scatter(X_pca[top_10_indices, 0], X_pca[top_10_indices, 1], c='red', s=50, alpha=0.5, label='Anomalies Détectées')\n",
    "\n",
    "for i in top_10_indices:\n",
    "    circle = plt.Circle((X_pca[i, 0], X_pca[i, 1]), result_lof[i], color='red', fill=False, linestyle='dashed')\n",
    "    plt.gca().add_patch(circle)\n",
    "    plt.text(X_pca[i, 0], X_pca[i, 1]+0.3, f\"{result_lof[i]:.2f}\", fontsize=8, color='red', ha='center', va='center')\n",
    "\n",
    "\n",
    "# Configuration du graphique\n",
    "plt.title('Détection d\\'anomalies avec LOF')\n",
    "plt.xlabel('1e composante principale')\n",
    "plt.ylabel('2e composante principale')\n",
    "plt.legend()\n",
    "\n",
    "# Personnalisation de l'arrière-plan et de la grille\n",
    "plt.gca().set_facecolor('lightgrey')\n",
    "plt.grid(color='white', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# Affichage du graphique\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIncx25Q5e1A"
   },
   "source": [
    "Résultat attendu :     \n",
    "```\n",
    "Pourcentage de bonnes classifications : 97.69%\n",
    "Matrice de confusion :\n",
    "------------------------\n",
    "| pred/true |  0  |  1  |\n",
    "------------------------\n",
    "|     0     |   7 |   3 |\n",
    "------------------------\n",
    "|     1     |   3 | 247 |\n",
    "------------------------\n",
    "```\n",
    "\n",
    "<center>\n",
    "<img src=\"https://drive.google.com/uc?id=1UfNGdoVnYaRHxqRQmrOBejZ_MmEc01HZ\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APd0nUe7Ei-B"
   },
   "source": [
    "### 3 - One-Class SVM\n",
    "\n",
    "Le One-Class SVM est une variante des SVM adaptée à la détection d'anomalies. Nous avons vu en cours qu'il nécessite de positionner deux hyperparamètres :    \n",
    "-   $\\gamma$ qui conditionne le noyau gaussien\n",
    "-   $\\nu$ qui est une borne supérieure du nombre de données considérées comme des anomalies.\n",
    "\n",
    "Il est naturel de poser $\\nu = \\frac{10}{260}$. Pour $\\gamma$, vous pouvez implémenter la formule d'Aggarwal : $\\gamma = \\frac{1}{2 \\sigma^2}$ où $\\sigma$ est la médiane des distances entre toutes les données.\n",
    "\n",
    "**Travail à faire** :    \n",
    "-   Implémentez le calcul de $\\gamma$ par la formule d'Aggarwal et affichez le résultat obtenu.\n",
    "-   Testez d'autres valeurs de $\\gamma$ et cherchez celle qui convient le mieux au problème d'après vous.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zomHQy2Ce6GY"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "# Calcul de gamma par la formule d'Aggarwal (reprise du code de la partie LOF)\n",
    "\n",
    "\n",
    "# Entraînement d'un SVM avec noyau gaussien\n",
    "oc_svm_model = ...\n",
    "\n",
    "# Evaluation du modèle\n",
    "y_pred = oc_svm_model.predict(X_pca)\n",
    "y_pred = np.where(y_pred<0, 0, 1)\n",
    "evaluate(y_pred,Y)\n",
    "\n",
    "# Affichage de la frontière de décision\n",
    "plot_decision_boundary(X_pca, Y, oc_svm_model, \"One-Class SVM à marge souple et noyau gaussien\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRVAsDka88Co"
   },
   "source": [
    "### 4 - Isolation Forests\n",
    "\n",
    "Dans cette partie qui vous est laissée libre, il vous revient de tester les forêts d'isolation. Cherchez comment exécutez cet algorithme dans scikit-learn et testez différentes valeurs d'hyperparamètrs pour identifier ceux qui vous donnent les meilleurs résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3MzxCVyE4kUL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
