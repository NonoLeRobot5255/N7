{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Dans ce dernier TP d'apprentissage, nous allons expérimenter du traitement du langage naturel à travers deux tâches classiques du domaines : la **classification de texte** et la **génération de texte**."
      ],
      "metadata": {
        "id": "DBlkTreEQS04"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Présentation des données"
      ],
      "metadata": {
        "id": "p7k2_Q2Bcevc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Commencez par télécharger les données :"
      ],
      "metadata": {
        "id": "-Tmi5HK0QjOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://acarlier.fr/tp/surname-nationality.csv"
      ],
      "metadata": {
        "id": "36H07MywchIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cette base de données regroupe des noms de famille usuels (les plus communs) dans 37 pays du monde. Le fichier CSV contient plus de 36000 noms (*surnames*) accompagnés de la nationalité associée. Bien évidemment, un nom peut apparaître dans plusieurs pays du monde (par exemple, le nom Lopez est d'après cette base très usuel au Honduras, au Chili, au Brésil, au Venezuela, au Nicaragua, au Pérou, et en Espagne !)."
      ],
      "metadata": {
        "id": "KVMw_6NEQmfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "# Initialisation de listes pour ranger les noms et nationalités\n",
        "surnames = []\n",
        "nationalities = []\n",
        "\n",
        "# Lecture du fichier CSV\n",
        "with open('surname-nationality.csv', 'r', newline='') as file:\n",
        "    reader = csv.reader(file)\n",
        "    next(reader)  # Header\n",
        "    for row in reader:\n",
        "        surnames.append(row[1].lower())\n",
        "        nationalities.append(row[2])\n",
        "\n",
        "print(surnames)\n",
        "print(nationalities)"
      ],
      "metadata": {
        "id": "K7bIBFUid-VY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification de texte"
      ],
      "metadata": {
        "id": "jf0nM2gXca6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans un premier temps, nous allons tenter de résoudre un problème de classification de texte : est-il possible de prédire la nationalité d'une personne uniquement à partir de son nom ?\n",
        "\n",
        "Pour les raisons évoquées précédemment, certaines nationalités risquent d'être indissociables car de nombreux noms de familles usuels sont communs dans les mêmes pays. Ainsi, pour simplifier le problème, nous allons nous circonscrire à une dizaine de pays dont les noms sont de consonance suffisamment diverse :"
      ],
      "metadata": {
        "id": "6VoZymxQRxW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nationality_classes = ['Spanish', 'Korean', 'Portuguese', 'French', 'Arabic', 'Indian', 'Italian', 'Vietnamese', 'Irish', 'German']"
      ],
      "metadata": {
        "id": "AFluhaXJPdmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il nous faut donc préparer les données pour ce problème.\n",
        "\n",
        "Pour commencer, on extrait les listes de noms/nationalités correspondant aux nationalités listées ci-dessus, puis on associe un id unique à chaque nationalité : ce sera la classe à prédire."
      ],
      "metadata": {
        "id": "k-2TFRj4SpPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Création d'un dictionnaire pour associer chaque nationalité à un indice\n",
        "nationality_to_label = {nationality: idx for idx, nationality in enumerate(nationality_classes)}\n",
        "\n",
        "selected_surnames = []\n",
        "selected_nationalities = []\n",
        "# Sélection du sous-ensemble de couples noms/nationalités pour les pays sélectionnés\n",
        "for s, n in zip(surnames, nationalities):\n",
        "  if n in nationality_classes:\n",
        "    selected_surnames.append(s)\n",
        "    selected_nationalities.append(n)\n",
        "\n",
        "# Génération des labels associés à ces nationalités\n",
        "labels = [nationality_to_label[nationality] for nationality in selected_nationalities]\n",
        "\n",
        "# Conversion de la liste en tableau numpy\n",
        "Y = np.array(labels)\n",
        "\n",
        "print(\"Labels (Y):\")\n",
        "print(Y)\n",
        "print(Y.shape)\n",
        "print(\"\\nDictionnaire Nationalité -> Indice\")\n",
        "print(nationality_to_label)"
      ],
      "metadata": {
        "id": "CessXCGMIUBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il nous faut maintenant préparer les données $X$. Cette partie est plus compliquée car elle nécessite deux étapes importantes :     \n",
        "1.   Convertir les caractères textuels en données numériques\n",
        "2.   Gérer le problème des séquences de longueur variable\n",
        "\n",
        "Pour la conversion en données numériques, il faut d'abord décider de l'unité de modélisation du texte : on parle de *tokens*. Ici, il semble assez naturel de considérer chaque caractère du nom comme un *token*. On va ensuite associer un indice à chaque caractère.\n",
        "\n",
        "Le problème des séquences de longueur variable est un problème à cause des structures de données que nous utilisons. Si chaque nom est codé par une séquence d'entiers, notre ensemble de données $X$ va donc être représenté par une matrice où chaque ligne correspond à un nom et compte donc un nombre potentiellement différent d'entiers sur les colonnes. Cela n'est pas possible car les tableaux Numpy nécessitent que toutes les lignes aient le même nombre de colonnes.\n",
        "\n",
        "On va donc adopter une solution simple qui consiste à détecter la longueur maximale d'une séquence, et à faire en sorte que toutes les séquences soient complétées par un *token* spécial (que l'on va appeler **pad**) pour atteindre la longueur maximale.\n",
        "\n",
        "On commence donc par trouver la longueur maximale d'une séquence :"
      ],
      "metadata": {
        "id": "Y4RhwON4iykr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 0\n",
        "# Parcours des noms du dataset\n",
        "for s in selected_surnames:\n",
        "  # A COMPLETER\n",
        "\n",
        "print(MAX_LEN)"
      ],
      "metadata": {
        "id": "tFlcIg5Ai15O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On détermine ensuite le vocabulaire, c'est-à-dire l'ensemble des caractères présents dans la base de données, auxquels on va assigner un indice de 1 à *vocab\\_size* (taille du vocabulaire). L'indice 0 correspondra au caractère **pad** que l'on utilisera pour compléter toutes les séquences à la longueur de la séquence maximale."
      ],
      "metadata": {
        "id": "rhSy7jg0I79P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concaténation de tous les noms du dataset\n",
        "all_names = ''\n",
        "for s in selected_surnames:\n",
        "  all_names = all_names + s\n",
        "\n",
        "# Détermination des caractères apparaissant au moins une fois, et tri de ces caractères\n",
        "vocab = sorted(set(all_names))\n",
        "# Ajout d'un token pour la complétion des séquences (padding)\n",
        "vocab.insert(0, '<pad>')\n",
        "\n",
        "# Création d'un dictionnaire pour associer chaque token à un indice\n",
        "char_to_id = {char: idx for idx, char in enumerate(vocab)}\n",
        "\n",
        "# Taille du vocabulaire final\n",
        "VOCAB_SIZE = len(vocab)\n",
        "\n",
        "print(vocab)\n",
        "print(char_to_id)"
      ],
      "metadata": {
        "id": "WMjsnZ0ReihZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il reste donc à générer la séquence associée à chaque nom. Pour cela on doit prendre en compte le fait que la longueur maximale d'une séquence est de 18, et que chaque caractère est codé par un entier :     \n",
        "\n",
        "|  \\<pad\\>  |       |   '   |  ,    |  a    |  b    |  c    |  ...  |\n",
        "|---    |:-:    |:-:    |:-:    |--:    |--:    |--:    |--:    |\n",
        "|  0    |    1  |   2   |   3   |   4   |   5   |   6   |  ...  |\n",
        "\n",
        "Ainsi, le nom \"o'sullivan\" est réécrit comme la séquence de *tokens* suivante :\n",
        "\n",
        "``` <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> o ' s u l l i v a n ```  \n",
        "\n",
        "qui va donc être codée ainsi :\n",
        "$[0, 0, 0, 0, 0, 0, 0, 0, 18, 2, 22, 24, 15, 15, 12, 25, 4, 17]$"
      ],
      "metadata": {
        "id": "sO-6cr4AWceb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.zeros((Y.shape[0], MAX_LEN))\n",
        "\n",
        "for idx, s in enumerate(selected_surnames):\n",
        "  s_id = [0] * (MAX_LEN - len(s)) + [char_to_id[char] for char in s] # On ajoute le bon nombre de 0 au début de la séquence\n",
        "  X[idx, :] = np.array(s_id)\n",
        "\n",
        "print(X)"
      ],
      "metadata": {
        "id": "Nz60EzqCjvB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On dispose donc maintenant de nos données $X$ et de nos étiquettes $Y$, il reste à les séparer en deux ensembles d'apprentissage et de test :"
      ],
      "metadata": {
        "id": "fKgSNJ7YuOVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.10, random_state=42)"
      ],
      "metadata": {
        "id": "ffwXCUpATfvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le problème de classification de texte nécessite d'associer un label à une séquence de caractères : c'est donc un problème *Many-to-one*.\n",
        "\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=1dkccEF--TaRvLMr0zFFQJQQdFYu7wMzF\" width=600> </center>\n",
        "<caption><center> Schéma d'une classification de nom à l'aide d'une cellule récurrente</center></caption>\n",
        "\n",
        "Nous allons construire le réseau le plus simple possible pour résoudre ce problème. Ce réseau sera composé de 3 couches :\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=1FiYjHu-YZS9bYZv5GBxSIGPsC-nEJJvA\" width=600> </center>\n",
        "<caption><center> Réseau à construire</center></caption>\n",
        "\n",
        "\n",
        "1.   Une couche d'[Embedding](https://keras.io/api/layers/core_layers/embedding/) pour transformer chaque *token* en un vecteur de dimension *embedding\\_size*.\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=13OG0O6-_7BdOKnvYQCsQM5PxZKcmAe95\" width=300> </center>\n",
        "\n",
        "2.   Une couche [récurrente](https://keras.io/api/layers/recurrent_layers/simple_rnn/) simple comportant un nombre de neurones à définir.\n",
        "\n",
        "3.   Une couche de sortie (Dense) qui va réaliser la classification.\n",
        "\n",
        "\n",
        "Vous pouvez utiliser la même valeur pour *embedding\\_size* et le nombre de neurones de la couche récurrente, par exemple 128."
      ],
      "metadata": {
        "id": "qWB6NyhJuat8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras import models\n",
        "\n",
        "# A COMPLETER\n",
        "model = models.Sequential()\n",
        "model.add(...)\n",
        "model.add(...)\n",
        "model.add(...)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "cRhsJW1cJwvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention, comme on l'a vu dans le TP5, il faut formater les labels en *one-hot vectors*"
      ],
      "metadata": {
        "id": "lw7p5edBA3in"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train_cat = keras.utils.to_categorical(Y_train, num_classes=10)\n",
        "Y_test_cat = keras.utils.to_categorical(Y_test, num_classes=10)\n"
      ],
      "metadata": {
        "id": "ZBh4FvrNBLPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A COMPLETER\n",
        "model.compile(loss=...,\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train, Y_train_cat, validation_data=(X_test, Y_test_cat), epochs=30, batch_size=32)"
      ],
      "metadata": {
        "id": "cj4S_FUSLQB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bien que le modèle apprenne toujours après 30 epochs, on observe un début de sur-apprentissage et il est donc raisonnable de s'arrêter là. Les résultats sont plutôt bons, jusqu'à 68% de bonnes classifications sur l'ensemble de test. Evidemment le réseau est encore probablement trop simple pour obtenir de meilleurs résultats."
      ],
      "metadata": {
        "id": "ufJtaLHvGjik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Version avec LSTM\n",
        "\n",
        "On a vu en cours que les cellules récurrentes avaient des difficultés à apprendre les dépendances à long terme, et que l'on pouvait utiliser des LSTM pour pallier à ce problème. Essayez de remplacer la cellule RNN par un LSTM et voyez la différence :"
      ],
      "metadata": {
        "id": "49lz2BQzlSEb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On observe en effet que le modèle généralise un peu mieux !"
      ],
      "metadata": {
        "id": "vLjft2yzCNoc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Génération de texte"
      ],
      "metadata": {
        "id": "rZrPYw7Llj-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passons maintenant à la génération de texte : on va créer un modèle de langage qui pourra ensuite nous aider à générer de nouveaux noms. Dans cette partie, on ne s'intéresse plus à la nationalité.\n",
        "\n",
        "Le problème est ici différent, de la classe *Many-to-Many*.\n",
        "\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=1TfgprY0yB4blIlRbHHr3S8UQB3nid5zo\" width=600> </center>\n",
        "<caption><center> Génération de nom à l'aide d'une cellule récurrente</center></caption>\n",
        "\n",
        "Le formatage des données est similaire à la partie précédente mais on va ajouter deux nouveaux *tokens* : l'un pour marquer le début d'un nom (\\<sos\\>, *start of surname*) et l'autre pour en marquer la fin (\\<eos\\>, *end of surname*).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Tkk0GakFlmHl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cette fois-ci nous allons utiliser tous les noms de la base initiale, il faut donc recalculer la longueur maximale d'une séquence :"
      ],
      "metadata": {
        "id": "nK5zU_hE7BdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 0\n",
        "# Parcours des noms du dataset\n",
        "for s in surnames:\n",
        "  if len(s) > MAX_LEN:\n",
        "    MAX_LEN = len(s)\n",
        "\n",
        "print(MAX_LEN)"
      ],
      "metadata": {
        "id": "S-XC_H81llHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On regénère également un vocabulaire (il y a un nouveau caractère, le '-', qui apparait dans certaines langues) en y adjoignant les deux nouveaux tokens.\n",
        "\n",
        "|  \\<pad\\>  |       |   '   |  ,    |  -   |  a    |  b    |  c    |  ...  |   z    |   \\<sos\\>    |   \\<eos\\>   |\n",
        "|---    |:-:    |:-:    |:-:    |:-:    |:--:    |:--:    |:--:    |:--:    |:--:    |:--:    |:--:    |\n",
        "|  0    |    1  |   2   |   3   |   4   |   5   |   6   |   7   |  ...  |   30    |  31   |   32  |  \n"
      ],
      "metadata": {
        "id": "JRnxv-Xa7Lch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concaténation de tous les noms du dataset\n",
        "all_names = ''\n",
        "for s in surnames:\n",
        "  all_names = all_names + s\n",
        "\n",
        "# Détermination des caractères apparaissant au moins une fois, et tri de ces caractères\n",
        "vocab = sorted(set(all_names))\n",
        "# Ajout d'un token pour la complétion des séquences (padding), ainsi que pour\n",
        "# le début et la fin des noms\n",
        "vocab.insert(0, '<pad>')\n",
        "vocab.append('<sos>') # Start of surname\n",
        "vocab.append('<eos>') # End of surname\n",
        "\n",
        "# Création d'un dictionnaire pour associer chaque token à un indice\n",
        "char_to_id = {char: idx for idx, char in enumerate(vocab)}\n",
        "# Taille du vocabulaire final\n",
        "VOCAB_SIZE = len(vocab)\n",
        "\n",
        "print(vocab)\n",
        "print(char_to_id)"
      ],
      "metadata": {
        "id": "uFYf0pcW7KuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autre différence avec la classification de texte, ici les labels $Y$ sont également des séquences ! Pour la génération de texte, nous allons ajouter les *tokens* de padding à la fin et non au début.\n",
        "\n",
        "\n",
        "Ainsi, le nom \"o'sullivan\" est réécrit comme la séquence de *tokens* suivante :\n",
        "\n",
        "``` <sos> o ' s u l l i v a n <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> ```  \n",
        "\n",
        "qui va donc être codée ainsi :\n",
        "$[31, 19, 2, 23, 25, 16, 16, 13, 26, 5, 18, 32, 0, 0, 0, 0, 0, 0, 0, 0]$\n",
        "\n",
        "Cette séquence est donc de taille MAX_LEN + 2 (on a ajouté deux *tokens* au début et à la fin du nom).\n",
        "\n",
        "La donnée $X$ associée à cette séquence est donc le vecteur $[31, 19, 2, 23, 25, 16, 16, 13, 26, 5, 18, 32, 0, 0, 0, 0, 0, 0, 0]$ des MAX_LEN + 1 premiers caractères, et le label $Y$ à prédire est le vecteur $[19, 2, 23, 25, 16, 16, 13, 26, 5, 18, 32, 0, 0, 0, 0, 0, 0, 0, 0]$ des MAX_LEN + 1 derniers caractères."
      ],
      "metadata": {
        "id": "WH3bkGJC8znI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Préparation des structures de donnée X et Y\n",
        "X = np.zeros((len(surnames), MAX_LEN+1))\n",
        "Y = np.zeros((len(surnames), MAX_LEN+1))\n",
        "\n",
        "for idx, s in enumerate(surnames):\n",
        "  # A COMPLETER\n",
        "\n",
        "\n",
        "print(X)\n",
        "print(Y)"
      ],
      "metadata": {
        "id": "Svvm2axq8yOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A nouveau, il ne faut pas oublier de transformer les labels en *one-hot vectors* :"
      ],
      "metadata": {
        "id": "w8NNgKe6MV7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A COMPLETER\n",
        "Y_cat = ..."
      ],
      "metadata": {
        "id": "jXX9UJg5IfbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous allons construire un réseau similaire à celui de la classification, excepté que cette fois il y aura une sortie pour chaque élément de la séquence d'entrée :   \n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=1KDA28fEeLwM5_bZUOifJNK-b3YdDF7WT\" width=600> </center>\n",
        "<caption><center> Réseau à construire</center></caption>\n",
        "\n",
        "Pour cela, lorsque vous mettre en place la couche récurrente (utilisez directement un LSTM, qui fonctionne mieux), positionnez correctement l'attribut ```return_sequences```."
      ],
      "metadata": {
        "id": "EzgJS0ehMbQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras import models\n",
        "\n",
        "# A COMPLETER\n",
        "model = models.Sequential()\n",
        "...\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "kfpGOcQaJAkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'apprentissage est un peu plus long (même pour seulement 10 epochs), donc n'hésitez pas à prendre le temps de lire le bloc de code suivant (pour générer des noms) pour ne pas perdre de temps."
      ],
      "metadata": {
        "id": "T1kwoNPlHT1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A COMPLETER\n",
        "model.compile(loss=...,\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X, Y_cat, epochs=10, batch_size=32)"
      ],
      "metadata": {
        "id": "tsQypjxZJNRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Une fois le modèle entraîné, nous pouvons maintenant générer un nouveau nom, soit de zéro ou soit en partant d'un début de séquence *seed* (**le code est fourni, prenez le temps de le comprendre !**).\n",
        "\n",
        "Notez bien que l'on utilise ce modèle de langage de manière stochastique. Plutôt que de choisir systématiquement le caractère le plus probable, on tire aléatoirement un caractère en suivant la distribution de probabilité prédite par le réseau, ce qui permet de générer plusieurs fins de nom possibles pour un même début de séquence ! (A vous de tester un peu)"
      ],
      "metadata": {
        "id": "G0ZVOg6JKOIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Début de séquence : mettre '' si l'on veut générer un nom de zéro\n",
        "seed_seq = 'macro'\n",
        "\n",
        "# Création de la séquence qui va être fournie en entrée du réseau :\n",
        "# On ajoute un token <sos> au démarrage, et on transcrit en la séquence d'id correspondante\n",
        "input_seq = [char_to_id['<sos>']]\n",
        "for s in seed_seq:\n",
        "  input_seq.append(char_to_id[s])\n",
        "\n",
        "last_char = -1\n",
        "i = 0\n",
        "\n",
        "# On génère des séquences de taille inférieure à MAX_LEN, et on s'arrête lorsque\n",
        "# l'on génère un token <eos> (id 32)\n",
        "while i <= MAX_LEN and last_char != 32:\n",
        "  # La séquence d'entrée doit être de dimension BATCH_SIZE x SEQ_LEN x 1\n",
        "  # soit en fait ici 1 x SEQ_LEN x 1\n",
        "  input = np.array(input_seq)\n",
        "  input = np.expand_dims(input, 0)\n",
        "  input = np.expand_dims(input, 2)\n",
        "\n",
        "  # Prédiction du modèle sur la séquence en cours\n",
        "  pred = model.predict(input, verbose=0)\n",
        "\n",
        "  # Échantillonnage du caractère généré à partir de la distribution de probabilité\n",
        "  # prédite par le modèle pour le dernier élément de la séquence\n",
        "  last_char = np.random.choice(33, 1, p=pred[0, -1])[0]\n",
        "\n",
        "  # Ajout du caractère à la séquence générée\n",
        "  input_seq.append(last_char)\n",
        "  i += 1\n",
        "\n",
        "# Affichage du nom généré\n",
        "generated_surname = ''\n",
        "for s in input_seq:\n",
        "  if s != 31 and s != 32:\n",
        "    generated_surname+=vocab[s]\n",
        "\n",
        "print(generated_surname)"
      ],
      "metadata": {
        "id": "pm90kHtSKMxB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}