{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>TP: Dynamic Programming</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h2>1. Introduction: Packet Scheduling</h2>\n",
    "\n",
    "Consider a router handling packets from flows of $K$ different priority classes. \n",
    "At a given moment, there is a fixed number $N_k$ of packets for each class $k=1,\\dots,K$. \n",
    "Therefore, the total number of packets is $N = \\sum_{k=1}^K N_k$.\n",
    "\n",
    "Packets of priority class $k$ have length $L_k$ [bits], such that $L_1 < L_2 < \\dots < L_K$.\n",
    "The router forwards the packets through a link of capacity $R$ [bps].\n",
    "Therefore, a packet of class $k$ takes $T_k=\\frac{L_k}{R}$ [s] to be transmitted. We refer to $T_k$ as class $k$'s <i>transmission delay</i>.\n",
    "\n",
    "Our goal is to <i>schedule</i> these packets for transmission.\n",
    "In other words, we wish to iteratively select one of the $N$ packets to transmit until all packets are transmitted. \n",
    "A system (algorithm) that is in charge of doing that is called a (packet) <i>scheduler</i>.\n",
    "\n",
    "Schedulers have specific performance targets.\n",
    "In this exercise, we are interested in the average wait time $W$ [s] experienced by the packets.\n",
    "The total wait time is defined as:<br>\n",
    "$W = \\sum_{n=1}^N W(n),$<br>\n",
    "where $W(n)$ is the wait time of the packet transmitted at iteration $n$.\n",
    "\n",
    "To understand the definition of a packet's wait time $W(n)$, consider that, at iteration $n\\in\\{1,\\dots,N\\}$, a packet will be selected for transmission.\n",
    "This packet will experience a transmission delay of $T(n)$.\n",
    "Note that $T(n)=T_k$ if the packet selected at iteration $n$ is from priority class $k$.\n",
    "\n",
    "In this context, the first packet $n=1$ will experience no wait time, i.e., $W(1)=0$.\n",
    "The second packet will experience the wait time of the first packet plus the transmission delay of the first packet, i.e., $W(2)=W(1)+T(1) = T(1)$.\n",
    "The third packet will experience the wait time of the second packet plus the transmission delay of the second packet, i.e., $W(3)=W(2)+T(2) = T(1) + T(2)$.\n",
    "In general, we have that <br>\n",
    "$W(n)=\\begin{cases} 0, & \\text{if } n=1, \\\\ W(n-1) + T(n-1), & \\text{otherwise.}\\end{cases}$\n",
    "\n",
    "The goal of this activity is to design a scheduler, i.e., to find a <u><b>scheduling policy</b></u> $\\pi^*$, that is able to minimize the average wait time of the packets in the described system.\n",
    "We call it the <i>Packet Scheduling Problem</i>. <br>\n",
    "<b>Note</b>: This is a static scenario where the number of packets is fixed in the beginning and no new packets arrive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2. MDP for Packet Scheduling</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2.1. Mathematical Model </h3>\n",
    "\n",
    "The first step is to represent the problem as an MDP with the following components: <br>\n",
    "- Set of states $\\mathcal S$  <br>\n",
    "- Set of actions $\\mathcal A$ <br>\n",
    "- State transition probabilities $\\mathcal P_{\\mathbf{ss}'}^a=\\mathbb{P}(S_{n+1}=\\mathbf{s}' | S_{n}=\\mathbf{s}, A_n=a), \\forall \\mathbf{s,s}'\\in\\mathcal S, \\forall a\\in\\mathcal A$ <br>\n",
    "- Reward function $\\mathcal R_s^a=\\mathbb{E}[R_{n+1} | S_n=\\mathbf s, A_t=a]$ <br>\n",
    "- Discount factor $\\gamma\\in[0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4> 2.1.1. Set of states</h4>\n",
    "\n",
    "For this exercise, we define each state as the number of packets of each class still to be transmitted, i.e., $\\mathcal S=\\{\\mathbf{s}=(n_1,n_2,\\dots,n_K)\\}_{n_k=1,\\dots,N_k, k=1,\\dots,K}$.\n",
    "The MDP has an initial state $\\mathbf{s}_0=(N_1,N_2,\\dots,N_K)$ and a terminal (absorbing) state $\\mathbf{s}'=(0,0,\\dots, 0)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 2.1.2. Set of actions </h4>\n",
    "\n",
    "At every iteration, we will first choose one priority class and transmit one packet of the selected class.\n",
    "Therefore, the action is defined over the set of classes, i.e., $\\mathcal A=\\{k\\}_{k=1,\\dots,K}$.\n",
    "Generically, at iteration $n$, the action $a_n\\in\\mathcal A$ is a transformation: <br>\n",
    "\n",
    "$a_n : \\mathcal S\\rightarrow \\mathcal S$, such that $a_n(\\mathbf{s}) = \\mathbf{s} \\ominus \\mathbf{e}_{k(n)}$,<br>\n",
    "\n",
    "where <br>\n",
    "- $k(n)$ is the class chosen at iteration $n$ and $\\mathbf{e}_{k(n)} \\in \\{0,1\\}^{1\\times K}$ is a vector whose entry $k$ is $1$ and all other entries are $0$, and<br>\n",
    "- $\\ominus$ operator is a modified vectorial subtraction operator, where any resulting negative entry is converted to zero.\n",
    "\n",
    "For example, we have $K=2$ classes and $N_1=N_2=3$ packets for each class. From the initial state $\\mathbf{s}_0=(N_1,N_2) = (3,3)$ , if we choose to transmit a packet from class $1$, we apply the transformation $a_1(\\cdot)$ as follows, <br>\n",
    "\n",
    "$a_1(\\mathbf{s}_0) = \\mathbf{s}_0 \\ominus \\mathbf{e}_1 = (3,3) \\ominus (1,0) = (2,3)$. <br>\n",
    "\n",
    "Therefore, if we are in state $\\mathbf{s}_0=(N_1,N_2) = (3,3)$ and transmit a packet from class $1$, then we land at state $\\mathbf{s}'=(2,3)$.<br>\n",
    "On the other hand, if we are in state $\\mathbf s=(3,0)$ and decide to transmit a packet from class $2$, then $a_n(\\mathbf{s}) = (3,0) \\ominus (0,1) = (3,0)$, i.e., no changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 2.1.3. State Transition Probabilities </h4>\n",
    "In this activity, all transmissions (actions) lead deterministically to the corresponding immediate smaller state, we have that the transition probabilities are:<br> \n",
    "\n",
    "$\\mathcal P_{s,s'}^{a} = \\begin{cases} 1, & \\text{ if } a(s)=s' \\\\ 0, & \\text{ otherwise}\\end{cases}$,  $\\forall s,s'\\in\\mathcal{S}, \\forall a\\in\\mathcal{A}$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For now, it is enough to describe these elements and, later, we will discuss about the reward signal and the discount factor.\n",
    "\n",
    "<b><u>Exercise 1:</u></b> Draw the MDP for the Packet Scheduling Problem for $K=2$ classes of priority and $3$ packets of each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2.2. Computational Model </h3>\n",
    "\n",
    "We start by importing a few auxiliary Python packages and initialize the random seed to zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below implements an MDP Python class for $K=2$ priority classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 1, 1, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "class MDP:\n",
    "    def __init__(self, classes=[3, 3]):        \n",
    "        self.K = 2              # Number of classes\n",
    "        self.N = classes        # Number of packets of each class\n",
    "\n",
    "        # set of states\n",
    "        self.S = [(i,j) for i in range(self.N[0],-1,-1) for j in range(self.N[1],-1,-1)]\n",
    "\n",
    "        # set of actions: Tx1 = transmits packet of class 1; Tx2 = transmits packet of class 2\n",
    "        self.A  = [\"Tx1\", \"Tx2\"]                                            \n",
    "\n",
    "        # definition of transition probabilities\n",
    "        self.P = { (s, ss, a): 1 if self.transmit(s,a) == ss else 0 for s in self.S for ss in self.S for a in self.A}  \n",
    "\n",
    "        self.packet_length  = [100e6, 100e7]                                    # Packet length [bits]\n",
    "        self.data_rate      = 100e6                                             # Data Rate [bps]\n",
    "        self.T              = [l / self.data_rate for l in self.packet_length]  # Transmission delay [s]\n",
    "\n",
    "        # generate queue with N1 and N2 packets randomly arranged\n",
    "        self.Q = random.sample([1] * self.N[0] + [2] * self.N[1], self.N[0] + self.N[1])\n",
    "        \n",
    "    # support function \"transmission\" that define the next state after a transmission,\n",
    "    # it helps determine the transition probabilities P and rewards R\n",
    "    def transmit(self, s, a):\n",
    "        if s == (0,0):\n",
    "            return s\n",
    "        if a == \"Tx1\":\n",
    "            return (max(0, s[0]-1), s[1])\n",
    "        if a == \"Tx2\":\n",
    "            return (s[0], max(0, s[1]-1))\n",
    "\n",
    "# Instance of MDP object\n",
    "mdp = MDP(classes=[3, 3])\n",
    "print(mdp.Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><u>Exercise 2:</u></b> <br>\n",
    "(a) The set of rewards $\\{\\mathcal{R}_{s}^a\\}_{a\\in\\mathcal{A}, s\\in\\mathcal{S}}$ is still missing from the MDP definition in Section 2.1. How would you choose the reward for this activity? Recall that there must be a reward for every action you take at every state. Explain your reasoning and add it the computational model in the code below.<br>\n",
    "(b) What about the discount factor $\\gamma$? Is there an appropriate value? Explain your reasoning and add it the computational model in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewards\n",
    "mdp.R =  -(length(mdp.S) -1) * mdp.A # TODO: Exercise 2(a): add the definition of reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discount factor, gamma\n",
    "mdp.g = 1 # TODO: Exercise 2(b): add value for the discount factor         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3. Testing Scheduling Policies </h2>\n",
    "\n",
    "Now, we will evaluate the performance of different policies.\n",
    "\n",
    "<h3> 3.1. Policy Evaluation </h3>\n",
    "\n",
    "The method we will use is the Policy Evaluation (PE), which relies on the iterative application of Bellman Expectation Equation: <br>\n",
    "\n",
    "$v_{\\pi}(s) = \\sum_{a\\in\\mathcal{A}} \\pi(a|\\,s) \\left[\\mathcal{R}_s^a + \\gamma \\sum_{s'\\in\\mathcal{S}} \\mathcal{P}_{ss'}^a\\cdot v_{\\pi}(s')\\right]$\n",
    "\n",
    "![pe](./policy_evaluation.png)\n",
    "\n",
    "<b><u>Exercise 3:</b></u> Use the Bellman Expectation Equation to finish the implementation of the PE function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Evaluation Algorithm\n",
    "# Input:  MDP parameters (states S, actions A, rewards R, transition probabilities Tp, discount factor gamma)\n",
    "#         policy pi\n",
    "#         maximum number of iterations max_iter\n",
    "# Output: list of the max_iter values for the states (last item of the list estimates v_pi)\n",
    "\n",
    "def policy_eval(mdp, pi, max_iter=100):\n",
    "    # algorithm parameters\n",
    "    theta = 1e-6\n",
    "\n",
    "    # initialize output variable with arbitrary values and zero for the terminal state, e.g., set all state values to zero\n",
    "    v = {s: 0.0 for s in mdp.S}\n",
    "\n",
    "    # main loop\n",
    "    for i in range(max_iter):\n",
    "        delta = 0.0\n",
    "        for s in mdp.S:\n",
    "            v_old = v[s]\n",
    "            v[s] = # TODO: Use bellman expectation equation to compute the value function of state s\n",
    "            delta = max(delta, abs(v_old - v[s]))\n",
    "            if delta < theta:\n",
    "                return v\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3.2. RANDOM Policy </h3>\n",
    "\n",
    "The first policy we will try is the RANDOM policy, which selects the next action uniformly at random from the set of available actions at every state.\n",
    "\n",
    "The implementation of a RANDOM policy is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pi_RANDOM = { (s, a): 1.0/len(mdp.A) for s in mdp.S for a in mdp.A }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we evaluate the RANDOM policy using the policy evaluation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the values of the random policy\n",
      "-88.68750\t-61.18750\t-42.62500\t-33.00000\n",
      "-61.18750\t-35.75000\t-19.25000\t-11.00000\n",
      "-42.62500\t-19.25000\t-5.50000\t 0.00000\n",
      "-33.00000\t-11.00000\t 0.00000\t 0.00000\n"
     ]
    }
   ],
   "source": [
    "# evaluate policy Pi over max_iter iterations and store value functions of every iteration\n",
    "v_RANDOM = policy_eval(mdp, Pi_RANDOM, max_iter=100)\n",
    "\n",
    "## print results\n",
    "print(\"These are the values of the random policy\")\n",
    "for s in mdp.S:\n",
    "    print(\" %.5f\" %v_RANDOM[s] if v_RANDOM[s]==0 else \"%.5f\" %v_RANDOM[s], end='\\n' if s[1] == 0 else '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><b>Exercise 4:</b></u> Explain the state values obtained for RANDOM\n",
    "\n",
    "<h3> 3.3. FIFO Policy </h3>\n",
    "\n",
    "In First-In, First-Out (FIFO) scheduling discipline, we process the packets in order they are organized in the queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pi_FIFO = {}\n",
    "for s in mdp.S:\n",
    "    if s == (0,0):\n",
    "        Pi_FIFO[(s, \"Tx1\")] = 0.5\n",
    "        Pi_FIFO[(s, \"Tx2\")] = 0.5\n",
    "    else:\n",
    "        for a in mdp.A:\n",
    "            Pi_FIFO[(s, a)] = 1.0  if \"Tx%d\" %mdp.Q[mdp.N[0] + mdp.N[1] - s[0] - s[1]] == a else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we evaluate the FIFO policy and are able to compare it with the RANDOM one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the values of the FIFO policy\n",
      "-96.00000\t-46.00000\t-6.00000\t-3.00000\n",
      "-47.00000\t-8.00000\t-3.00000\t-1.00000\n",
      "-11.00000\t-6.00000\t-1.00000\t 0.00000\n",
      "-10.00000\t-5.00000\t 0.00000\t 0.00000\n"
     ]
    }
   ],
   "source": [
    "# evaluate policy Pi over max_iter iterations and store value functions of every iteration\n",
    "v_FIFO = policy_eval(mdp, Pi_FIFO, max_iter=1000)\n",
    "\n",
    "## print results\n",
    "print(\"These are the values of the FIFO policy\")\n",
    "for s in mdp.S:\n",
    "    print(\" %.5f\" %v_FIFO[s] if v_FIFO[s]==0 else \"%.5f\" %v_FIFO[s], end='\\n' if s[1] == 0 else '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><b>Exercise 5:</b></u> Explain the state values obtained for FIFO. Is there any scenario where FIFO provides good performance?\n",
    "\n",
    "\n",
    "<h3> 3.4. Student's Policy </h3>\n",
    "\n",
    "<u><b>Exercise 6:</b></u> Design your own policy and compare it with RANDOM and FIFO.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define your own policy and compare it with RANDOM and FIFO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 5. Finding the Optimal Scheduling Policy </h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 5.1. Value Iteration</h3>\n",
    "\n",
    "The method we will use is called the Value Iteration (VI), which relies on the iterative application of Bellman Optimality Equation: <br>\n",
    "\n",
    "$v_t(s) = \\max_{a\\in\\mathcal{A}} \\left[\\mathcal{R}_s^a + \\gamma \\sum_{s'\\in\\mathcal{S}} \\mathcal{P}_{ss'}^a\\cdot v_{t-1}(s')\\right]$\n",
    "\n",
    "![vi](./value_iteration.png)\n",
    "\n",
    "<b><u>Exercise 7:</b></u> Use the Bellman Optimality Equation to finish the implementation of the VI function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Evaluation Algorithm\n",
    "# Input:  MDP parameters (states S, actions A, rewards R, transition probabilities Tp, discount factor gamma)\n",
    "#         maximum number of iterations max_iter\n",
    "# Output: list of the max_iter values for the states (last item of the list estimates v_pi)\n",
    "\n",
    "def value_iteration(mdp, max_iter=100):\n",
    "\n",
    "    # algorithm parameters\n",
    "    theta = 1e-6\n",
    "\n",
    "    # initialize output variable with arbitrary values and zero for the terminal state, e.g., set all state values to zero\n",
    "    v = {s: 0.0 for s in mdp.S}\n",
    "\n",
    "    # main loop\n",
    "    for i in range(max_iter):\n",
    "        delta = 0.0\n",
    "        break_flag = False\n",
    "        for s in mdp.S:\n",
    "            v_old = v[s]\n",
    "            v[s] = # TODO: Use bellman optimality equation to compute the estimated optimal value function of state s\n",
    "            delta = max(delta, abs(v_old - v[s]))\n",
    "            if delta < theta:\n",
    "                break_flag = True\n",
    "                break\n",
    "        if break_flag:\n",
    "            break\n",
    "    \n",
    "    best_actions = {}\n",
    "    for s in mdp.S:\n",
    "        best_actions[s] = \"Tx1\" if v[mdp.transmit(s, \"Tx1\")] > v[mdp.transmit(s, \"Tx2\")] else \"Tx2\"\n",
    "    Pi = {(s, a): 1 if a == best_actions[s] else 0 for s in mdp.S for a in mdp.A}\n",
    "\n",
    "    return v, Pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 5.2. Testing Value Iteration </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the values of the FIFO policy\n",
      " -96.00000\t -46.00000\t -6.00000\t -3.00000\n",
      " -47.00000\t -8.00000\t -3.00000\t -1.00000\n",
      " -11.00000\t -6.00000\t -1.00000\t 0.00000\n",
      " -10.00000\t -5.00000\t 0.00000\t 0.00000\n",
      "\n",
      "These are the values of the Greedy policy\n",
      " -42.00000\t -19.00000\t -6.00000\t -3.00000\n",
      " -37.00000\t -15.00000\t -3.00000\t -1.00000\n",
      " -33.00000\t -12.00000\t -1.00000\t 0.00000\n",
      " -30.00000\t -10.00000\t 0.00000\t 0.00000\n"
     ]
    }
   ],
   "source": [
    "# print optimal value obtained by Policy Iteration\n",
    "print(\"These are the values of the FIFO policy\")\n",
    "for s in mdp.S:\n",
    "    print(\" %.5f\" %v_FIFO[s], end='\\n' if s[1] == 0 else '\\t')\n",
    "    \n",
    "# compute optimal value for the MDP\n",
    "vi_start = time.time()\n",
    "v_OPT, Pi_OPT = value_iteration(mdp, max_iter=1000)\n",
    "vi_end = time.time()\n",
    "\n",
    "# print optimal value obtained by Value Iteration\n",
    "print(\"\\nThese are the values of the Greedy policy\")\n",
    "for s in mdp.S:\n",
    "    print(\" %.5f\" %v_OPT[s], end='\\n' if s[1] == 0 else '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><u>Exercise 8:</b></u> Using the results obtained from the application of value iteration, answer: In a simple rule, what is the optimal policy for the Scheduling Problem?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
